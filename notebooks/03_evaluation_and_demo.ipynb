{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOgR2KR8eMkTajnGjzS92gs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAI11/fragranceBERT/blob/main/notebooks/03_evaluation_and_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYyPRwGE3pha",
        "outputId": "55bfc11f-9f99-485f-c2aa-c95365353106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers scikit-learn torch pandas numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39upVGJi4CT3",
        "outputId": "2d578726-3cf0-4d8f-9348-cb6f8c8b8557"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "os.makedirs('./data', exist_ok=True)\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "print('\\nupload files:')\n",
        "print('1. perfumes_with_ids.csv')\n",
        "print('2. perfume_embeddings.npy')\n",
        "print('3. test.csv')\n",
        "print('4. fragrance-retriever.zip (model)')\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# handle each file\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        # extract zip to models directory\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('./models/')\n",
        "        print(f'extracted {filename} to ./models/')\n",
        "\n",
        "        # check what was extracted\n",
        "        extracted_files = os.listdir('./models/')\n",
        "        print(f'extracted files: {extracted_files}')\n",
        "    else:\n",
        "        # move data files\n",
        "        target_path = f'./data/{filename}'\n",
        "        with open(target_path, 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "        print(f'moved {filename} to ./data/')\n",
        "\n",
        "print('\\nupload complete')\n",
        "\n",
        "# verify model directory structure\n",
        "print('\\nmodel directory structure:')\n",
        "for root, dirs, files in os.walk('./models/'):\n",
        "    level = root.replace('./models/', '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files:\n",
        "        print(f'{subindent}{file}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "Ynhpqrwn4Efv",
        "outputId": "729073c0-6fa4-46ff-e6ea-800288a1b402"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "upload files:\n",
            "1. perfumes_with_ids.csv\n",
            "2. perfume_embeddings.npy\n",
            "3. test.csv\n",
            "4. fragrance-retriever.zip (model)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c1a9ab42-40a5-4d41-9b19-bca6fa8079e4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c1a9ab42-40a5-4d41-9b19-bca6fa8079e4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving fragrance-retriever.zip to fragrance-retriever (1).zip\n",
            "Saving perfume_embeddings.npy to perfume_embeddings.npy\n",
            "Saving perfumes_with_ids.csv to perfumes_with_ids.csv\n",
            "Saving test.csv to test.csv\n",
            "extracted fragrance-retriever (1).zip to ./models/\n",
            "extracted files: ['2_Normalize', 'sentence_bert_config.json', 'model.safetensors', 'config_sentence_transformers.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'README.md', 'config.json', '1_Pooling', 'modules.json', 'eval', 'vocab.txt']\n",
            "moved perfume_embeddings.npy to ./data/\n",
            "moved perfumes_with_ids.csv to ./data/\n",
            "moved test.csv to ./data/\n",
            "\n",
            "upload complete\n",
            "\n",
            "model directory structure:\n",
            "/\n",
            "  sentence_bert_config.json\n",
            "  model.safetensors\n",
            "  config_sentence_transformers.json\n",
            "  special_tokens_map.json\n",
            "  tokenizer.json\n",
            "  tokenizer_config.json\n",
            "  README.md\n",
            "  config.json\n",
            "  modules.json\n",
            "  vocab.txt\n",
            "2_Normalize/\n",
            "1_Pooling/\n",
            "  config.json\n",
            "eval/\n",
            "  Information-Retrieval_evaluation_val_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perfumes_df = pd.read_csv('./data/perfumes_with_ids.csv', index_col=0)\n",
        "perfume_embeddings = np.load('./data/perfume_embeddings.npy')\n",
        "test_df = pd.read_csv('./data/test.csv')\n",
        "\n",
        "print(f'\\nloaded {len(perfumes_df)} perfumes')\n",
        "print(f'embeddings shape: {perfume_embeddings.shape}')\n",
        "print(f'test set: {len(test_df)} examples')\n",
        "\n",
        "# load trained model - files are directly in ./models/\n",
        "model = SentenceTransformer('./models/')\n",
        "print(f'loaded model: {model.get_sentence_embedding_dimension()}-dim embeddings')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzIkBXfH4Q-2",
        "outputId": "a80547e2-a551-4c3a-a6ca-c31b7497f827"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "loaded 24063 perfumes\n",
            "embeddings shape: (24063, 384)\n",
            "test set: 1500 examples\n",
            "loaded model: 384-dim embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hand-crafted realistic queries for comprehensive evaluation\n",
        "eval_queries = [\n",
        "    \"warm cozy scent for winter mornings by the fireplace\",\n",
        "    \"fresh citrus for spring afternoons\",\n",
        "    \"romantic floral for date night\",\n",
        "    \"professional clean scent for office\",\n",
        "    \"sweet vanilla cookies baking\",\n",
        "    \"masculine woody leather\",\n",
        "    \"sensual amber evening\",\n",
        "    \"energizing morning coffee and bergamot\",\n",
        "    \"summer beach coconut and salt\",\n",
        "    \"elegant powdery iris\",\n",
        "    \"spicy cinnamon autumn\",\n",
        "    \"calming lavender bedtime\",\n",
        "    \"confident oud and tobacco\",\n",
        "    \"playful fruity peach\",\n",
        "    \"sophisticated rose and musk\"\n",
        "]\n",
        "\n",
        "print(f'\\nevaluation queries: {len(eval_queries)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Uq1nW8O4SRU",
        "outputId": "d0660487-4c60-48fe-b90a-c2bce68c812d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "evaluation queries: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# baseline 1: random selection\n",
        "# ============================================================================\n",
        "\n",
        "class RandomBaseline:\n",
        "    \"\"\"randomly select perfumes - worst case baseline\"\"\"\n",
        "\n",
        "    def __init__(self, perfumes_df):\n",
        "        self.perfumes_df = perfumes_df\n",
        "        self.indices = list(range(len(perfumes_df)))\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[int]:\n",
        "        \"\"\"return random perfume indices\"\"\"\n",
        "        return random.sample(self.indices, top_k)"
      ],
      "metadata": {
        "id": "xk6UJFND4UqA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# baseline 2: tf-idf with cosine similarity\n",
        "# ============================================================================\n",
        "\n",
        "class TfidfBaseline:\n",
        "    \"\"\"traditional information retrieval with tf-idf\"\"\"\n",
        "\n",
        "    def __init__(self, perfumes_df):\n",
        "        self.perfumes_df = perfumes_df\n",
        "        self.descriptions = perfumes_df['description'].tolist()\n",
        "\n",
        "        # fit tf-idf vectorizer\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=5000,\n",
        "            ngram_range=(1, 2),\n",
        "            stop_words='english'\n",
        "        )\n",
        "        self.doc_vectors = self.vectorizer.fit_transform(self.descriptions)\n",
        "        print('tf-idf vectorizer fitted')\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[int]:\n",
        "        \"\"\"search using tf-idf cosine similarity\"\"\"\n",
        "        query_vector = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_vector, self.doc_vectors)[0]\n",
        "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "        return top_indices.tolist()"
      ],
      "metadata": {
        "id": "mH6aw8A44ZCi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# baseline 3: keyword matching with rules\n",
        "# ============================================================================\n",
        "\n",
        "class KeywordBaseline:\n",
        "    \"\"\"rule-based keyword matching\"\"\"\n",
        "\n",
        "    def __init__(self, perfumes_df):\n",
        "        self.perfumes_df = perfumes_df\n",
        "        self.descriptions = perfumes_df['description'].str.lower().tolist()\n",
        "\n",
        "        # keyword categories\n",
        "        self.keywords = {\n",
        "            'vanilla': ['vanilla', 'vanille'],\n",
        "            'citrus': ['citrus', 'lemon', 'orange', 'bergamot', 'grapefruit'],\n",
        "            'floral': ['floral', 'rose', 'jasmine', 'lily', 'iris', 'violet'],\n",
        "            'woody': ['woody', 'wood', 'cedar', 'sandalwood', 'vetiver'],\n",
        "            'fresh': ['fresh', 'aquatic', 'marine', 'water'],\n",
        "            'spicy': ['spicy', 'cinnamon', 'pepper', 'ginger', 'cardamom'],\n",
        "            'sweet': ['sweet', 'honey', 'caramel', 'sugar'],\n",
        "            'leather': ['leather', 'suede'],\n",
        "            'amber': ['amber', 'resin'],\n",
        "            'musk': ['musk', 'musky']\n",
        "        }\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[int]:\n",
        "        \"\"\"match keywords and score perfumes\"\"\"\n",
        "        query_lower = query.lower()\n",
        "        scores = np.zeros(len(self.descriptions))\n",
        "\n",
        "        # score based on keyword matches\n",
        "        for desc_idx, desc in enumerate(self.descriptions):\n",
        "            for category, keywords in self.keywords.items():\n",
        "                # check if query mentions this category\n",
        "                category_in_query = any(kw in query_lower for kw in keywords)\n",
        "                # check if description contains this category\n",
        "                category_in_desc = any(kw in desc for kw in keywords)\n",
        "\n",
        "                if category_in_query and category_in_desc:\n",
        "                    scores[desc_idx] += 1\n",
        "\n",
        "        # if no matches, return random\n",
        "        if scores.sum() == 0:\n",
        "            return random.sample(range(len(self.descriptions)), top_k)\n",
        "\n",
        "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "        return top_indices.tolist()"
      ],
      "metadata": {
        "id": "QwmWVBI14bKr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# initialize baselines\n",
        "# ============================================================================\n",
        "\n",
        "print('\\ninitializing baselines...')\n",
        "random_baseline = RandomBaseline(perfumes_df)\n",
        "tfidf_baseline = TfidfBaseline(perfumes_df)\n",
        "keyword_baseline = KeywordBaseline(perfumes_df)\n",
        "print('baselines ready')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZLDLdSD4dhQ",
        "outputId": "3c01f21e-b3d4-42ca-f00c-cee3e38b0e1e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "initializing baselines...\n",
            "tf-idf vectorizer fitted\n",
            "baselines ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# bi-encoder retrieval system\n",
        "# ============================================================================\n",
        "\n",
        "class BiEncoderRetriever:\n",
        "    \"\"\"trained sentence-bert retrieval\"\"\"\n",
        "\n",
        "    def __init__(self, model, perfume_embeddings, perfumes_df):\n",
        "        self.model = model\n",
        "        self.perfume_embeddings = perfume_embeddings\n",
        "        self.perfumes_df = perfumes_df\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[int]:\n",
        "        \"\"\"encode query and find nearest neighbors\"\"\"\n",
        "        query_embedding = self.model.encode([query], convert_to_tensor=False)\n",
        "        similarities = cosine_similarity(query_embedding, self.perfume_embeddings)[0]\n",
        "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "        return top_indices.tolist()\n",
        "\n",
        "bi_encoder = BiEncoderRetriever(model, perfume_embeddings, perfumes_df)\n",
        "print('bi-encoder retriever ready')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVwGoCzq4fQp",
        "outputId": "00bb6e66-e680-4ed2-c7f2-54eba2f1adee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bi-encoder retriever ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# evaluation metrics\n",
        "# ============================================================================\n",
        "\n",
        "def precision_at_k(relevant: set, retrieved: List, k: int) -> float:\n",
        "    \"\"\"fraction of top-k results that are relevant\"\"\"\n",
        "    retrieved_k = set(retrieved[:k])\n",
        "    if len(retrieved_k) == 0:\n",
        "        return 0.0\n",
        "    return len(relevant & retrieved_k) / k\n",
        "\n",
        "def mean_reciprocal_rank(relevant: set, retrieved: List) -> float:\n",
        "    \"\"\"inverse rank of first relevant item\"\"\"\n",
        "    for i, item in enumerate(retrieved, 1):\n",
        "        if item in relevant:\n",
        "            return 1.0 / i\n",
        "    return 0.0\n",
        "\n",
        "def ndcg_at_k(relevant: set, retrieved: List, k: int) -> float:\n",
        "    \"\"\"normalized discounted cumulative gain\"\"\"\n",
        "    dcg = 0.0\n",
        "    for i, item in enumerate(retrieved[:k], 1):\n",
        "        if item in relevant:\n",
        "            dcg += 1.0 / np.log2(i + 1)\n",
        "\n",
        "    # ideal dcg\n",
        "    idcg = sum(1.0 / np.log2(i + 1) for i in range(1, min(len(relevant), k) + 1))\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def evaluate_retrieval(retriever, test_df, k_values=[5, 10]):\n",
        "    \"\"\"evaluate retrieval system on test set\"\"\"\n",
        "\n",
        "    results = {\n",
        "        'precision@5': [],\n",
        "        'precision@10': [],\n",
        "        'mrr': [],\n",
        "        'ndcg@10': []\n",
        "    }\n",
        "\n",
        "    # group by query to get relevant perfumes\n",
        "    query_groups = test_df.groupby('query')['perfume_id'].apply(set).to_dict()\n",
        "\n",
        "    for query, relevant_ids in tqdm(query_groups.items(), desc='evaluating'):\n",
        "        # retrieve top-k\n",
        "        retrieved = retriever.search(query, top_k=10)\n",
        "\n",
        "        # calculate metrics\n",
        "        results['precision@5'].append(precision_at_k(relevant_ids, retrieved, 5))\n",
        "        results['precision@10'].append(precision_at_k(relevant_ids, retrieved, 10))\n",
        "        results['mrr'].append(mean_reciprocal_rank(relevant_ids, retrieved))\n",
        "        results['ndcg@10'].append(ndcg_at_k(relevant_ids, retrieved, 10))\n",
        "\n",
        "    # aggregate\n",
        "    return {k: np.mean(v) for k, v in results.items()}\n"
      ],
      "metadata": {
        "id": "Jd5p9GAb4hLZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# evaluate all systems\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('evaluating all systems on test set')\n",
        "print('='*80)\n",
        "\n",
        "systems = {\n",
        "    'bi-encoder': bi_encoder,\n",
        "    'tf-idf': tfidf_baseline,\n",
        "    'keyword matching': keyword_baseline,\n",
        "    'random': random_baseline\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for name, system in systems.items():\n",
        "    print(f'\\nevaluating {name}...')\n",
        "    results = evaluate_retrieval(system, test_df)\n",
        "    all_results[name] = results\n",
        "\n",
        "    print(f'  precision@5:  {results[\"precision@5\"]:.3f}')\n",
        "    print(f'  precision@10: {results[\"precision@10\"]:.3f}')\n",
        "    print(f'  mrr:          {results[\"mrr\"]:.3f}')\n",
        "    print(f'  ndcg@10:      {results[\"ndcg@10\"]:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf_kfp2L4jwQ",
        "outputId": "8cbee4b2-fb9b-4aa9-c4bd-70c6c6940538"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "evaluating all systems on test set\n",
            "================================================================================\n",
            "\n",
            "evaluating bi-encoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating: 100%|██████████| 1286/1286 [01:03<00:00, 20.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  precision@5:  0.047\n",
            "  precision@10: 0.028\n",
            "  mrr:          0.192\n",
            "  ndcg@10:      0.211\n",
            "\n",
            "evaluating tf-idf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating: 100%|██████████| 1286/1286 [00:23<00:00, 54.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  precision@5:  0.030\n",
            "  precision@10: 0.020\n",
            "  mrr:          0.104\n",
            "  ndcg@10:      0.125\n",
            "\n",
            "evaluating keyword matching...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating: 100%|██████████| 1286/1286 [08:49<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  precision@5:  0.001\n",
            "  precision@10: 0.001\n",
            "  mrr:          0.005\n",
            "  ndcg@10:      0.005\n",
            "\n",
            "evaluating random...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating: 100%|██████████| 1286/1286 [00:00<00:00, 75542.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  precision@5:  0.000\n",
            "  precision@10: 0.000\n",
            "  mrr:          0.000\n",
            "  ndcg@10:      0.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# results comparison table\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('results comparison')\n",
        "print('='*80)\n",
        "\n",
        "results_df = pd.DataFrame(all_results).T\n",
        "results_df = results_df[['precision@5', 'precision@10', 'mrr', 'ndcg@10']]\n",
        "print(results_df.to_string())\n",
        "\n",
        "# calculate improvements\n",
        "bi_encoder_results = all_results['bi-encoder']\n",
        "tfidf_results = all_results['tf-idf']\n",
        "\n",
        "improvements = {}\n",
        "for metric in bi_encoder_results.keys():\n",
        "    if tfidf_results[metric] > 0:\n",
        "        improvement = (bi_encoder_results[metric] - tfidf_results[metric]) / tfidf_results[metric] * 100\n",
        "        improvements[metric] = improvement\n",
        "\n",
        "print(f'\\nbi-encoder vs tf-idf improvements:')\n",
        "for metric, improvement in improvements.items():\n",
        "    print(f'  {metric}: +{improvement:.1f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZn-6RUb4l9J",
        "outputId": "a8445024-8a9d-4915-c26f-8f863f624ad0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "results comparison\n",
            "================================================================================\n",
            "                  precision@5  precision@10       mrr   ndcg@10\n",
            "bi-encoder           0.047278      0.027605  0.191644  0.211436\n",
            "tf-idf               0.030482      0.019518  0.103725  0.125231\n",
            "keyword matching     0.000933      0.000778  0.004518  0.005238\n",
            "random               0.000000      0.000000  0.000000  0.000000\n",
            "\n",
            "bi-encoder vs tf-idf improvements:\n",
            "  precision@5: +55.1%\n",
            "  precision@10: +41.4%\n",
            "  mrr: +84.8%\n",
            "  ndcg@10: +68.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# qualitative evaluation on hand crafted queries\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('qualitative evaluation on hand-crafted queries')\n",
        "print('='*80)\n",
        "\n",
        "def display_results(query: str, retriever, top_k: int = 3):\n",
        "    \"\"\"show top-k results for a query\"\"\"\n",
        "    print(f'\\nquery: \"{query}\"')\n",
        "    print('-' * 80)\n",
        "\n",
        "    indices = retriever.search(query, top_k)\n",
        "    for i, idx in enumerate(indices, 1):\n",
        "        perfume = perfumes_df.iloc[idx]\n",
        "        print(f'{i}. {perfume[\"Perfume\"]} by {perfume[\"Brand\"]}')\n",
        "\n",
        "        # extract accords\n",
        "        desc = perfume['description']\n",
        "        if 'accords:' in desc:\n",
        "            accords = desc.split('accords:')[1].split('.')[0].strip()\n",
        "            print(f'   accords: {accords}')\n",
        "\n",
        "# test on 5 diverse queries\n",
        "test_queries = [\n",
        "    \"warm vanilla for cozy winter evenings\",\n",
        "    \"fresh citrus for spring afternoons\",\n",
        "    \"romantic floral for date night\",\n",
        "    \"masculine woody leather\",\n",
        "    \"energizing morning coffee and bergamot\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    display_results(query, bi_encoder, top_k=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOjbbXpM4n5R",
        "outputId": "1790620c-1984-44d8-bd4b-cd6f794d2cb4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "qualitative evaluation on hand-crafted queries\n",
            "================================================================================\n",
            "\n",
            "query: \"warm vanilla for cozy winter evenings\"\n",
            "--------------------------------------------------------------------------------\n",
            "1. vanille-passion by comptoir-sud-pacifique\n",
            "   accords: vanilla, powdery, musky\n",
            "2. nature-s-sexy by linn-young\n",
            "   accords: vanilla, floral, fresh\n",
            "3. vanille by molinard\n",
            "   accords: vanilla, powdery, almond\n",
            "\n",
            "query: \"fresh citrus for spring afternoons\"\n",
            "--------------------------------------------------------------------------------\n",
            "1. fresh-life by fresh\n",
            "   accords: citrus, floral, fresh\n",
            "2. cologne-summer-flash by mugler\n",
            "   accords: citrus, green, fresh\n",
            "3. soleil-de-capri by montale\n",
            "   accords: citrus, fresh, white floral\n",
            "\n",
            "query: \"romantic floral for date night\"\n",
            "--------------------------------------------------------------------------------\n",
            "1. kenzo-amour-florale by kenzo\n",
            "   accords: citrus, white floral, floral\n",
            "2. elixir-charnel-floral-romantique by guerlain\n",
            "   accords: white floral, sweet, floral\n",
            "3. beautiful-magnolia by estee-lauder\n",
            "   accords: floral, musky, white floral\n",
            "\n",
            "query: \"masculine woody leather\"\n",
            "--------------------------------------------------------------------------------\n",
            "1. fabulous-mandawa by salvador-dali\n",
            "   accords: leather, woody, fruity\n",
            "2. homme by xerjoff\n",
            "   accords: leather, woody, warm spicy\n",
            "3. ch-men-hot-hot-hot by carolina-herrera\n",
            "   accords: citrus, woody, aromatic\n",
            "\n",
            "query: \"energizing morning coffee and bergamot\"\n",
            "--------------------------------------------------------------------------------\n",
            "1. chypre-shot by olfactive-studio\n",
            "   accords: warm spicy, amber, fresh spicy\n",
            "2. coffee-duo-woman by o-boticario\n",
            "   accords: warm spicy, lactonic, lavender\n",
            "3. very-irresistible-givenchy-fresh-attitude by givenchy\n",
            "   accords: aromatic, fresh spicy, citrus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ablation study: pretrained vs fine-tuned\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('ablation study: pretrained vs fine-tuned')\n",
        "print('='*80)\n",
        "\n",
        "# load pretrained model without fine-tuning\n",
        "pretrained_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# encode with pretrained model\n",
        "print('encoding with pretrained model...')\n",
        "pretrained_embeddings = pretrained_model.encode(\n",
        "    perfumes_df['description'].tolist(),\n",
        "    show_progress_bar=True,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "pretrained_retriever = BiEncoderRetriever(pretrained_model, pretrained_embeddings, perfumes_df)\n",
        "\n",
        "# evaluate\n",
        "print('\\nevaluating pretrained model...')\n",
        "pretrained_results = evaluate_retrieval(pretrained_retriever, test_df)\n",
        "\n",
        "print('\\ncomparison:')\n",
        "print(f'{\"metric\":<15} {\"pretrained\":<12} {\"fine-tuned\":<12} {\"improvement\":<12}')\n",
        "print('-' * 55)\n",
        "for metric in ['precision@5', 'precision@10', 'mrr', 'ndcg@10']:\n",
        "    pretrained_val = pretrained_results[metric]\n",
        "    finetuned_val = bi_encoder_results[metric]\n",
        "    improvement = (finetuned_val - pretrained_val) / pretrained_val * 100 if pretrained_val > 0 else 0\n",
        "    print(f'{metric:<15} {pretrained_val:<12.3f} {finetuned_val:<12.3f} +{improvement:<11.1f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679,
          "referenced_widgets": [
            "1df07282da704d72ba4cfa7c66c2e3d2",
            "db377046c31a4fdfaf2ed18dd01085de",
            "a44a0b416490477fb825e0d8e26b001b",
            "f168abaa2c784502a75794ab26425b29",
            "c532e368e45f46aead5d9806e3fd0336",
            "8b01c15a8ab545b99179a250ac1f042b",
            "114da8c363c0478e887429020338a009",
            "d7c2767c771e4bd888656f38c577af51",
            "d73a98b4478e4a7fbe5ec0372e5c850e",
            "9a033c0a556543a9a19e98908e196688",
            "62f6167cb00b45fbbacf6001a8ff953a",
            "050f7430197f4197bc5e555915441cc5",
            "538fdec7bcab430e9cde7946f21ecc76",
            "9440a538730b462893d250c8b28b518e",
            "9b0c46d2d7ca4edfa4c4e7b4a135821a",
            "96a8d1a8460748d38a568860cb47d5a6",
            "caa595fe4b514ef8b4112efdea350484",
            "44705b6da0af4a4db3c0015170632488",
            "d1840571c4d948a980795b02f693be0b",
            "9e51f4d3903444e3815eeb6c178aca71",
            "2f8ae18beba34b5bb537d43f1f3baffa",
            "6779ad12f634492cb11495391a223e46",
            "1d31f7d8ba004fa8ab7bf187b3fb810e",
            "9e3662eafdee4b7b9222e92c8852be3b",
            "e9e985e24460418c9706e80e05b33b81",
            "20f927dbb8844bd592331368e52a28c2",
            "a4ee53f435e042ce87bc63068bc4bf72",
            "dbe30b0606944feebaafe87ff8baa362",
            "1458bc120b0c4330b3572a12808711a2",
            "5f3dd4fb536b40dda044118d45578442",
            "64c28246975948ba9fe04c3b44c3c90e",
            "d5e58d7e5ea34f758fd4f0ab2692247f",
            "6a2b241d75c4434aa1b3b903b6b870f7",
            "8abafc0e029748d9851dead1df686596",
            "c2b6cccd74284e56968900ce75ba24d7",
            "1bf4047a143c42399dce6f6174240601",
            "4ef19743d1ac4b84836cea6a29528bf1",
            "553cca2ff30f46ce96449d2c54686288",
            "fc9dc769236d46d1ace0422589ced2b6",
            "f2abcfb339544996a53324f3ff6094dc",
            "601b920bb07c43f6950c1ab96a584cc8",
            "40eadcc25fe9421e87a02198f842903b",
            "b8705a1198a940679e744571d84fae24",
            "daaa671f82af457d9740a666fe4bf59e",
            "710271d17eec47a0aa5c44892e68c5e6",
            "c272476a5e5f434b85de481256724c3f",
            "1f2626be466a453589d333e889facc92",
            "e07f822dea594f8a9f22a44ee8e15b3d",
            "d838f2570d904649b9f7839cf77034ce",
            "ffa75b0328cf42119ff29346af1c8bb7",
            "3ec52c215c7945ae9992ecb44fbd1738",
            "73a737f3323e4ee08637afe8d7087031",
            "e225f91c5267433797f2368f36864fd9",
            "8a203f3b4b1b4260b0ef0f2affef8c9e",
            "10b3c2cacbf346cfab02255d0a1f82d1",
            "b792dfe6ea144e76875cb2c291c87c78",
            "c7dabb4c2d8b4dcdbc22d512b5d055a9",
            "fee088f2c8914531b15ebf69e5032a1a",
            "2ef0adc4869447148a01fc4ba1b398a4",
            "41b8fb2a7bd0492eaa60e73e27b75165",
            "de73038e3a7b4bbc818f6dea49f1175a",
            "b7518aeafbaf4b47b44097cededf36dc",
            "14af66b20ebc428387071ee27db67a6b",
            "9d296a4fea734278ad156c094c1943b4",
            "a211433fae0a49a08064d3497c1a2e0e",
            "7f16850b16324b439520941756b73510",
            "2e50628ad0614d0d869a2827d13bd832",
            "2fb1655028dc430f8851a94141cd32f7",
            "b458352bedae423aaa1b827a03141052",
            "8dc8a13976454f5f9c22ccd134b12bc8",
            "98a1792905c947738bdf2f507142f6ab",
            "ad8b7e40b8284306b399bb7ef09852d6",
            "98091233820a466ba3c87ae459da15fb",
            "00b16fd68f7e4a8da75f5f77fbb0d7c6",
            "d9f7225e88554deeb9657fd4dd535190",
            "6f35d88cfb6948b49d40c78f528692be",
            "05d1a9cf45884c1f873c7093668da76b",
            "c987a97ab4284da9b59d3d87d6f99a93",
            "b0566590f4134748aab5eb18261d51a3",
            "3c6af69801e54c6e8c7ee62a48386be8",
            "16ddfc12c6774e9988334cfb48e85710",
            "fede30c5b410498f86f4ad90abad4aeb",
            "47f73854d52c41849649e00d4d8df169",
            "5d1881f31ebd49f3a9d3bf2db7b95a3d",
            "b62514c5e18846dcb44b0d27c9018d23",
            "32db1dd551ca4623b811477076ccbf49",
            "079e4515bb124e58a0cf30e25460b2c0",
            "3a9931a1e9d84fd1bd4532ce8dc150bb",
            "dc031567291e4711b7feb9c62451bd76",
            "c16f9f55e08b43e784b43801099ec1da",
            "6d638a01e58949249d35e37be1df7e40",
            "558556b79c9e46c2a20b14f4587a8d52",
            "e17087f915f440bf924e6c67f7fc184d",
            "693cbe6ba6f44e5089a59bc4cea47a75",
            "79b69e0dde5b4b11947afc1c8a609658",
            "d845479afd5543b596038d4d56bb3f25",
            "113d2d1435ad4ab5af337d3e0d7a1ab9",
            "efaf8968a5e843b7bf33b3e6a1150620",
            "c71e6bcc8bda4d59b49271e322b5c525",
            "aa9eede2d259403c8cb591feac81e73f",
            "b92d9f952c4343e58c7d3e984967edbb",
            "9670b910cf4549a997acc290aab58de3",
            "280eb3f11ec346a5866f0ca03d01a4dd",
            "8f27a5ff9623466997e0813c401c49ff",
            "477c2bf5a1be4acebdf55c14b2a854ee",
            "5838e0af369c42328576496fd32da9fc",
            "1f5d0dbbed3d4b46ac2a78f193ce0453",
            "9b933229421448468bc38c81fb9d6a7a",
            "396fc4dc1ff543cbbb03a9e5ace52d18",
            "fcdc334a62eb4f1c93a116efffd8b91e",
            "0ab1134f8aae41a685eca31e5ddc02ef",
            "1aee679b71b84ee38360e98ebc97e384",
            "e06f732c533e4d57b1f56e66f406c852",
            "47c4539dc8214fbfa39419cbf61a1dbc",
            "d86119eb2095429fbd1c6d4f81b64bf2",
            "bf56e7bcdb3a4ee7a5e29a5c6c440c8c",
            "0be130d005a74e8fb9e637fd0a73aab6",
            "51df41b154804ae393613735371a563c",
            "05605bd6e2d648be88a33b9e85f60466",
            "e5d8b2e95432440aaaaabd217d586cc3",
            "38f89a7708f444ea90f00b44ae85c1f7",
            "dadefa1afb0f4bae84933c8199ac3eae",
            "e88346a8f73548d4a45fe152a9f635cf",
            "1a62d342a3254bee8a4cda4697249b1c",
            "5a292d1a677e4ada9f41bd2e112e7030",
            "1c2cdb91af49446a913dba64281662b8",
            "73ccc9745a75402cb7ecf2f2aedf400c",
            "17c97e336343465bb886ca78d9cb09a9",
            "5a588c25b8ac48c1be0a35935a592aa1",
            "87d73dc1865c4976903589f81e485fee",
            "e511c95e92784bd0bb435a6e24fd0f5d",
            "86d29f9512e14eef87056d46441a6deb"
          ]
        },
        "id": "w5PxrxzJ4qNT",
        "outputId": "63de7c4e-2885-4101-9eec-b3d021dfbc46"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ablation study: pretrained vs fine-tuned\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1df07282da704d72ba4cfa7c66c2e3d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "050f7430197f4197bc5e555915441cc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d31f7d8ba004fa8ab7bf187b3fb810e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8abafc0e029748d9851dead1df686596"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "710271d17eec47a0aa5c44892e68c5e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b792dfe6ea144e76875cb2c291c87c78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e50628ad0614d0d869a2827d13bd832"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c987a97ab4284da9b59d3d87d6f99a93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc031567291e4711b7feb9c62451bd76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa9eede2d259403c8cb591feac81e73f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ab1134f8aae41a685eca31e5ddc02ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoding with pretrained model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/376 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dadefa1afb0f4bae84933c8199ac3eae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "evaluating pretrained model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating: 100%|██████████| 1286/1286 [01:03<00:00, 20.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "comparison:\n",
            "metric          pretrained   fine-tuned   improvement \n",
            "-------------------------------------------------------\n",
            "precision@5     0.026        0.047        +81.0       %\n",
            "precision@10    0.015        0.028        +84.9       %\n",
            "mrr             0.106        0.192        +80.4       %\n",
            "ndcg@10         0.116        0.211        +81.8       %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}